{
  
    
        "post0": {
            "title": "Install Prerequisites",
            "content": "Remove existing CUDA installation and NVIDIA drivers (optional) . # !dpkg -l | grep cuda- | awk &#39;{print $2}&#39; | xargs -n1 dpkg --purge # !apt-get remove cuda-* # !apt autoremove # !apt-get update . Install specific CUDA (optional) . # !dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb # !apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub # !apt-get update # !apt-get install cuda-9.2 . Installation check . !nvcc --version . nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2020 NVIDIA Corporation Built on Mon_Oct_12_20:09:46_PDT_2020 Cuda compilation tools, release 11.1, V11.1.105 Build cuda_11.1.TC455_06.29190527_0 . Install a jupyter extension . !pip install git+git://github.com/j143/nvcc4jupyter.git . Collecting git+git://github.com/j143/nvcc4jupyter.git Cloning git://github.com/j143/nvcc4jupyter.git to /tmp/pip-req-build-bmsddg1m Running command git clone -q git://github.com/j143/nvcc4jupyter.git /tmp/pip-req-build-bmsddg1m Building wheels for collected packages: NVCCPlugin Building wheel for NVCCPlugin (setup.py) ... done Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4306 sha256=8e5c1d33f097359faac19a33ddab1dc8b0c99dcd1957490a2c6652f8b0a6e62a Stored in directory: /tmp/pip-ephem-wheel-cache-40gt56am/wheels/26/f6/b6/abe58d118498a098d0c925b2011902a9f1b4a50629ef215768 Successfully built NVCCPlugin Installing collected packages: NVCCPlugin Successfully installed NVCCPlugin-0.0.2 . Load the plugin . %load_ext nvcc_plugin . created output directory at /content/src Out bin /content/result.out . Run CUDA . Run a print example . %%cu #include &lt;iostream&gt; int main() { std::cout &lt;&lt; &quot;This is from CUDA n&quot;; return 0; } . This is from CUDA . Involved example . int(&#39;0b100&#39;, base=0) . 4 . %%cu #include &lt;cstdio&gt; #include &lt;iostream&gt; using namespace std; __global__ void maxi(int* a, int* b, int n) { int block = 256 * blockIdx.x; int max = 0; for (int i = block; i &lt; min(256 + block, n); i++) { if (max &lt; a[i]) { max = a[i]; } } b[blockIdx.x] = max; } int main() { int n; n = 3 &gt;&gt; 2; int a[n]; for (int i = 0; i &lt; n; i++) { a[i] = rand() % n; cout &lt;&lt; a[i] &lt;&lt; &quot; t&quot;; } cudaEvent_t start, end; int *ad, *bd; int size = n * sizeof(int); cudaMalloc(&amp;ad, size); cudaMemcpy(ad, a, size, cudaMemcpyHostToDevice); int grids = ceil(n * 1.0f / 256.0f); cudaMalloc(&amp;bd, grids * sizeof(int)); dim3 grid(grids, 1); dim3 block(1, 1); cudaEventCreate(&amp;start); cudaEventCreate(&amp;end); cudaEventRecord(start); while (n &gt; 1) { maxi&lt;&lt;&lt;grids, block&gt;&gt;&gt;(ad, bd, n); n = ceil(n * 1.0f / 256.0f); cudaMemcpy(ad, bd, n * sizeof(int), cudaMemcpyDeviceToDevice); } cudaEventRecord(end); cudaEventSynchronize(end); float time = 0; cudaEventElapsedTime(&amp;time, start, end); int ans[2]; cudaMemcpy(ans, ad, 4, cudaMemcpyDeviceToHost); cout &lt;&lt; &quot;The maximum element is : &quot; &lt;&lt; ans[0] &lt;&lt; endl; cout &lt;&lt; &quot;The time required : &quot;; cout &lt;&lt; time &lt;&lt; endl; } . The maximum element is : -1365408560 The time required : 0.003264 . Example 1 . %%cu #include &lt;stdio.h&gt; // This is a special function that runs on the GPU (device) instead of the CPU (host) __global__ void kernel() { printf(&quot;Hello world! n&quot;); } int main() { // Invoke the kernel function on the GPU with one block of one thread kernel&lt;&lt;&lt;1,1&gt;&gt;&gt;(); // Check for error codes (remember to do this for _every_ CUDA function) if(cudaDeviceSynchronize() != cudaSuccess) { fprintf(stderr, &quot;CUDA Error: %s n&quot;, cudaGetErrorString(cudaPeekAtLastError())); } return 0; } . Hello world! . Example 2 . %%cu #include &lt;stdio.h&gt; // This kernel runs on the GPU and prints the thread&#39;s identifiers __global__ void kernel() { printf(&quot;Hello from block %d thread %d n&quot;, blockIdx.x, threadIdx.x); } int main() { // Launch the kernel on the GPU with four blocks of six threads each kernel&lt;&lt;&lt;4,2&gt;&gt;&gt;(); // Check for CUDA errors if(cudaDeviceSynchronize() != cudaSuccess) { fprintf(stderr, &quot;CUDA Error: %s n&quot;, cudaGetErrorString(cudaPeekAtLastError())); } return 0; } . Hello from block 2 thread 0 Hello from block 2 thread 1 Hello from block 0 thread 0 Hello from block 0 thread 1 Hello from block 3 thread 0 Hello from block 3 thread 1 Hello from block 1 thread 0 Hello from block 1 thread 1 . Example 3 . %%cu #include &lt;stdint.h&gt; #include &lt;stdio.h&gt; #define N 32 #define THREADS_PER_BLOCK 32 __global__ void saxpy(float a, float* x, float* y) { // Which index of the array should this thread use? size_t index = 20; // Compute a times x plus y for a specific index y[index] = a * x[index] + y[index];Z } int main() { // Allocate arrays for X and Y on the CPU. This memory is only usable on the CPU float* cpu_x = (float*)malloc(sizeof(float) * N); float* cpu_y = (float*)malloc(sizeof(float) * N); // Initialize X and Y int i; for(i=0; i&lt;N; i++) { cpu_x[i] = (float)i; cpu_y[i] = 0.0; } // The gpu_x and gpu_y pointers will only be usable on the GPU (which uses separate memory) float* gpu_x; float* gpu_y; // Allocate space for the x array on the GPU if(cudaMalloc(&amp;gpu_x, sizeof(float) * N) != cudaSuccess) { fprintf(stderr, &quot;Failed to allocate X array on GPU n&quot;); exit(2); } // Allocate space for the y array on the GPU if(cudaMalloc(&amp;gpu_y, sizeof(float) * N) != cudaSuccess) { fprintf(stderr, &quot;Failed to allocate Y array on GPU n&quot;); exit(2); } // Copy the cpu&#39;s x array to the gpu with cudaMemcpy if(cudaMemcpy(gpu_x, cpu_x, sizeof(float) * N, cudaMemcpyHostToDevice) != cudaSuccess) { fprintf(stderr, &quot;Failed to copy X to the GPU n&quot;); } // Copy the cpu&#39;s y array to the gpu with cudaMemcpy if(cudaMemcpy(gpu_y, cpu_y, sizeof(float) * N, cudaMemcpyHostToDevice) != cudaSuccess) { fprintf(stderr, &quot;Failed to copy Y to the GPU n&quot;); } // Calculate the number of blocks to run, rounding up to include all threads size_t blocks = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK; // Run the saxpy kernel saxpy&lt;&lt;&lt;blocks, THREADS_PER_BLOCK&gt;&gt;&gt;(0.5, gpu_x, gpu_y); // Wait for the kernel to finish if(cudaDeviceSynchronize() != cudaSuccess) { fprintf(stderr, &quot;CUDA Error: %s n&quot;, cudaGetErrorString(cudaPeekAtLastError())); } // Copy the y array back from the gpu to the cpu if(cudaMemcpy(cpu_y, gpu_y, sizeof(float) * N, cudaMemcpyDeviceToHost) != cudaSuccess) { fprintf(stderr, &quot;Failed to copy Y from the GPU n&quot;); } // Print the updated y array for(i=0; i&lt;N; i++) { printf(&quot;%d: %f n&quot;, i, cpu_y[i]); } cudaFree(gpu_x); cudaFree(gpu_y); free(cpu_x); free(cpu_y); return 0; } . /tmp/tmpzza2hszf/5bd0fb96-f0fd-4537-a788-0ddff5743a19.cu(13): error: identifier &#34;Z&#34; is undefined /tmp/tmpzza2hszf/5bd0fb96-f0fd-4537-a788-0ddff5743a19.cu(14): error: expected a &#34;;&#34; 2 errors detected in the compilation of &#34;/tmp/tmpzza2hszf/5bd0fb96-f0fd-4537-a788-0ddff5743a19.cu&#34;. .",
            "url": "https://j143.github.io/tech-insights/2022/08/06/another-notebook.html",
            "relUrl": "/2022/08/06/another-notebook.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Install Prerequisites",
            "content": "Remove existing CUDA installation and NVIDIA drivers (optional) . # !dpkg -l | grep cuda- | awk &#39;{print $2}&#39; | xargs -n1 dpkg --purge # !apt-get remove cuda-* # !apt autoremove # !apt-get update . Install specific CUDA (optional) . # !dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb # !apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub # !apt-get update # !apt-get install cuda-9.2 . Installation check . !nvcc --version . nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2020 NVIDIA Corporation Built on Mon_Oct_12_20:09:46_PDT_2020 Cuda compilation tools, release 11.1, V11.1.105 Build cuda_11.1.TC455_06.29190527_0 . Install a jupyter extension . !pip install git+git://github.com/j143/nvcc4jupyter.git . Collecting git+git://github.com/j143/nvcc4jupyter.git Cloning git://github.com/j143/nvcc4jupyter.git to /tmp/pip-req-build-bmsddg1m Running command git clone -q git://github.com/j143/nvcc4jupyter.git /tmp/pip-req-build-bmsddg1m Building wheels for collected packages: NVCCPlugin Building wheel for NVCCPlugin (setup.py) ... done Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4306 sha256=8e5c1d33f097359faac19a33ddab1dc8b0c99dcd1957490a2c6652f8b0a6e62a Stored in directory: /tmp/pip-ephem-wheel-cache-40gt56am/wheels/26/f6/b6/abe58d118498a098d0c925b2011902a9f1b4a50629ef215768 Successfully built NVCCPlugin Installing collected packages: NVCCPlugin Successfully installed NVCCPlugin-0.0.2 . Load the plugin . %load_ext nvcc_plugin . created output directory at /content/src Out bin /content/result.out . Run CUDA . Run a print example . %%cu #include &lt;iostream&gt; int main() { std::cout &lt;&lt; &quot;This is from CUDA n&quot;; return 0; } . This is from CUDA . Involved example . int(&#39;0b100&#39;, base=0) . 4 . %%cu #include &lt;cstdio&gt; #include &lt;iostream&gt; using namespace std; __global__ void maxi(int* a, int* b, int n) { int block = 256 * blockIdx.x; int max = 0; for (int i = block; i &lt; min(256 + block, n); i++) { if (max &lt; a[i]) { max = a[i]; } } b[blockIdx.x] = max; } int main() { int n; n = 3 &gt;&gt; 2; int a[n]; for (int i = 0; i &lt; n; i++) { a[i] = rand() % n; cout &lt;&lt; a[i] &lt;&lt; &quot; t&quot;; } cudaEvent_t start, end; int *ad, *bd; int size = n * sizeof(int); cudaMalloc(&amp;ad, size); cudaMemcpy(ad, a, size, cudaMemcpyHostToDevice); int grids = ceil(n * 1.0f / 256.0f); cudaMalloc(&amp;bd, grids * sizeof(int)); dim3 grid(grids, 1); dim3 block(1, 1); cudaEventCreate(&amp;start); cudaEventCreate(&amp;end); cudaEventRecord(start); while (n &gt; 1) { maxi&lt;&lt;&lt;grids, block&gt;&gt;&gt;(ad, bd, n); n = ceil(n * 1.0f / 256.0f); cudaMemcpy(ad, bd, n * sizeof(int), cudaMemcpyDeviceToDevice); } cudaEventRecord(end); cudaEventSynchronize(end); float time = 0; cudaEventElapsedTime(&amp;time, start, end); int ans[2]; cudaMemcpy(ans, ad, 4, cudaMemcpyDeviceToHost); cout &lt;&lt; &quot;The maximum element is : &quot; &lt;&lt; ans[0] &lt;&lt; endl; cout &lt;&lt; &quot;The time required : &quot;; cout &lt;&lt; time &lt;&lt; endl; } . The maximum element is : -1365408560 The time required : 0.003264 . Example 1 . %%cu #include &lt;stdio.h&gt; // This is a special function that runs on the GPU (device) instead of the CPU (host) __global__ void kernel() { printf(&quot;Hello world! n&quot;); } int main() { // Invoke the kernel function on the GPU with one block of one thread kernel&lt;&lt;&lt;1,1&gt;&gt;&gt;(); // Check for error codes (remember to do this for _every_ CUDA function) if(cudaDeviceSynchronize() != cudaSuccess) { fprintf(stderr, &quot;CUDA Error: %s n&quot;, cudaGetErrorString(cudaPeekAtLastError())); } return 0; } . Hello world! . Example 2 . %%cu #include &lt;stdio.h&gt; // This kernel runs on the GPU and prints the thread&#39;s identifiers __global__ void kernel() { printf(&quot;Hello from block %d thread %d n&quot;, blockIdx.x, threadIdx.x); } int main() { // Launch the kernel on the GPU with four blocks of six threads each kernel&lt;&lt;&lt;4,2&gt;&gt;&gt;(); // Check for CUDA errors if(cudaDeviceSynchronize() != cudaSuccess) { fprintf(stderr, &quot;CUDA Error: %s n&quot;, cudaGetErrorString(cudaPeekAtLastError())); } return 0; } . Hello from block 2 thread 0 Hello from block 2 thread 1 Hello from block 0 thread 0 Hello from block 0 thread 1 Hello from block 3 thread 0 Hello from block 3 thread 1 Hello from block 1 thread 0 Hello from block 1 thread 1 . Example 3 . %%cu #include &lt;stdint.h&gt; #include &lt;stdio.h&gt; #define N 32 #define THREADS_PER_BLOCK 32 __global__ void saxpy(float a, float* x, float* y) { // Which index of the array should this thread use? size_t index = 20; // Compute a times x plus y for a specific index y[index] = a * x[index] + y[index];Z } int main() { // Allocate arrays for X and Y on the CPU. This memory is only usable on the CPU float* cpu_x = (float*)malloc(sizeof(float) * N); float* cpu_y = (float*)malloc(sizeof(float) * N); // Initialize X and Y int i; for(i=0; i&lt;N; i++) { cpu_x[i] = (float)i; cpu_y[i] = 0.0; } // The gpu_x and gpu_y pointers will only be usable on the GPU (which uses separate memory) float* gpu_x; float* gpu_y; // Allocate space for the x array on the GPU if(cudaMalloc(&amp;gpu_x, sizeof(float) * N) != cudaSuccess) { fprintf(stderr, &quot;Failed to allocate X array on GPU n&quot;); exit(2); } // Allocate space for the y array on the GPU if(cudaMalloc(&amp;gpu_y, sizeof(float) * N) != cudaSuccess) { fprintf(stderr, &quot;Failed to allocate Y array on GPU n&quot;); exit(2); } // Copy the cpu&#39;s x array to the gpu with cudaMemcpy if(cudaMemcpy(gpu_x, cpu_x, sizeof(float) * N, cudaMemcpyHostToDevice) != cudaSuccess) { fprintf(stderr, &quot;Failed to copy X to the GPU n&quot;); } // Copy the cpu&#39;s y array to the gpu with cudaMemcpy if(cudaMemcpy(gpu_y, cpu_y, sizeof(float) * N, cudaMemcpyHostToDevice) != cudaSuccess) { fprintf(stderr, &quot;Failed to copy Y to the GPU n&quot;); } // Calculate the number of blocks to run, rounding up to include all threads size_t blocks = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK; // Run the saxpy kernel saxpy&lt;&lt;&lt;blocks, THREADS_PER_BLOCK&gt;&gt;&gt;(0.5, gpu_x, gpu_y); // Wait for the kernel to finish if(cudaDeviceSynchronize() != cudaSuccess) { fprintf(stderr, &quot;CUDA Error: %s n&quot;, cudaGetErrorString(cudaPeekAtLastError())); } // Copy the y array back from the gpu to the cpu if(cudaMemcpy(cpu_y, gpu_y, sizeof(float) * N, cudaMemcpyDeviceToHost) != cudaSuccess) { fprintf(stderr, &quot;Failed to copy Y from the GPU n&quot;); } // Print the updated y array for(i=0; i&lt;N; i++) { printf(&quot;%d: %f n&quot;, i, cpu_y[i]); } cudaFree(gpu_x); cudaFree(gpu_y); free(cpu_x); free(cpu_y); return 0; } . /tmp/tmpzza2hszf/5bd0fb96-f0fd-4537-a788-0ddff5743a19.cu(13): error: identifier &#34;Z&#34; is undefined /tmp/tmpzza2hszf/5bd0fb96-f0fd-4537-a788-0ddff5743a19.cu(14): error: expected a &#34;;&#34; 2 errors detected in the compilation of &#34;/tmp/tmpzza2hszf/5bd0fb96-f0fd-4537-a788-0ddff5743a19.cu&#34;. .",
            "url": "https://j143.github.io/tech-insights/2022/03/27/cuda-on-colab.html",
            "relUrl": "/2022/03/27/cuda-on-colab.html",
            "date": " • Mar 27, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "AlphaFold Colab",
            "content": "#@markdown Please execute this cell by pressing the _Play_ button #@markdown on the left to download and import third-party software #@markdown in this Colab notebook. (See the [acknowledgements](https://github.com/deepmind/alphafold/#acknowledgements) in our readme.) #@markdown **Note**: This installs the software on the Colab #@markdown notebook in the cloud and not on your computer. from IPython.utils import io import os import subprocess import tqdm.notebook TQDM_BAR_FORMAT = &#39;{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]&#39; try: with tqdm.notebook.tqdm(total=100, bar_format=TQDM_BAR_FORMAT) as pbar: with io.capture_output() as captured: # Uninstall default Colab version of TF. %shell pip uninstall -y tensorflow %shell sudo apt install --quiet --yes hmmer pbar.update(6) # Install py3dmol. %shell pip install py3dmol pbar.update(2) # Install OpenMM and pdbfixer. %shell rm -rf /opt/conda %shell wget -q -P /tmp https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; bash /tmp/Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda &amp;&amp; rm /tmp/Miniconda3-latest-Linux-x86_64.sh pbar.update(9) PATH=%env PATH %env PATH=/opt/conda/bin:{PATH} %shell conda update -qy conda &amp;&amp; conda install -qy -c conda-forge python=3.7 openmm=7.5.1 pdbfixer pbar.update(80) # Create a ramdisk to store a database chunk to make Jackhmmer run fast. %shell sudo mkdir -m 777 --parents /tmp/ramdisk %shell sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk pbar.update(2) %shell wget -q -P /content https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt pbar.update(1) except subprocess.CalledProcessError: print(captured) raise . #@markdown Please execute this cell by pressing the *Play* button on #@markdown the left. GIT_REPO = &#39;https://github.com/deepmind/alphafold&#39; SOURCE_URL = &#39;https://storage.googleapis.com/alphafold/alphafold_params_colab_2021-10-27.tar&#39; PARAMS_DIR = &#39;./alphafold/data/params&#39; PARAMS_PATH = os.path.join(PARAMS_DIR, os.path.basename(SOURCE_URL)) try: with tqdm.notebook.tqdm(total=100, bar_format=TQDM_BAR_FORMAT) as pbar: with io.capture_output() as captured: %shell rm -rf alphafold %shell git clone --branch main {GIT_REPO} alphafold pbar.update(8) # Install the required versions of all dependencies. %shell pip3 install -r ./alphafold/requirements.txt # Run setup.py to install only AlphaFold. %shell pip3 install --no-dependencies ./alphafold pbar.update(10) # Apply OpenMM patch. %shell pushd /opt/conda/lib/python3.7/site-packages/ &amp;&amp; patch -p0 &lt; /content/alphafold/docker/openmm.patch &amp;&amp; popd # Make sure stereo_chemical_props.txt is in all locations where it could be searched for. %shell mkdir -p /content/alphafold/alphafold/common %shell cp -f /content/stereo_chemical_props.txt /content/alphafold/alphafold/common %shell mkdir -p /opt/conda/lib/python3.7/site-packages/alphafold/common/ %shell cp -f /content/stereo_chemical_props.txt /opt/conda/lib/python3.7/site-packages/alphafold/common/ %shell mkdir --parents &quot;{PARAMS_DIR}&quot; %shell wget -O &quot;{PARAMS_PATH}&quot; &quot;{SOURCE_URL}&quot; pbar.update(27) %shell tar --extract --verbose --file=&quot;{PARAMS_PATH}&quot; --directory=&quot;{PARAMS_DIR}&quot; --preserve-permissions %shell rm &quot;{PARAMS_PATH}&quot; pbar.update(55) except subprocess.CalledProcessError: print(captured) raise import jax if jax.local_devices()[0].platform == &#39;tpu&#39;: raise RuntimeError(&#39;Colab TPU runtime not supported. Change it to GPU via Runtime -&gt; Change Runtime Type -&gt; Hardware accelerator -&gt; GPU.&#39;) elif jax.local_devices()[0].platform == &#39;cpu&#39;: raise RuntimeError(&#39;Colab CPU runtime not supported. Change it to GPU via Runtime -&gt; Change Runtime Type -&gt; Hardware accelerator -&gt; GPU.&#39;) else: print(f&#39;Running with {jax.local_devices()[0].device_kind} GPU&#39;) # Make sure everything we need is on the path. import sys sys.path.append(&#39;/opt/conda/lib/python3.7/site-packages&#39;) sys.path.append(&#39;/content/alphafold&#39;) # Make sure all necessary environment variables are set. import os os.environ[&#39;TF_FORCE_UNIFIED_MEMORY&#39;] = &#39;1&#39; os.environ[&#39;XLA_PYTHON_CLIENT_MEM_FRACTION&#39;] = &#39;2.0&#39; . Making a prediction . Please paste the sequence of your protein in the text box below, then run the remaining cells via Runtime &gt; Run after. You can also run the cells individually by pressing the Play button on the left. . Note that the search against databases and the actual prediction can take some time, from minutes to hours, depending on the length of the protein and what type of GPU you are allocated by Colab (see FAQ below). . #@markdown Enter the amino acid sequence(s) to fold: #@markdown * If you enter only a single sequence, the monomer model will be used. #@markdown * If you enter multiple sequences, the multimer model will be used. from alphafold.notebooks import notebook_utils sequence_1 = &#39;MAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH&#39; #@param {type:&quot;string&quot;} sequence_2 = &#39;&#39; #@param {type:&quot;string&quot;} sequence_3 = &#39;&#39; #@param {type:&quot;string&quot;} sequence_4 = &#39;&#39; #@param {type:&quot;string&quot;} sequence_5 = &#39;&#39; #@param {type:&quot;string&quot;} sequence_6 = &#39;&#39; #@param {type:&quot;string&quot;} sequence_7 = &#39;&#39; #@param {type:&quot;string&quot;} sequence_8 = &#39;&#39; #@param {type:&quot;string&quot;} input_sequences = (sequence_1, sequence_2, sequence_3, sequence_4, sequence_5, sequence_6, sequence_7, sequence_8) #@markdown If folding a complex target and all the input sequences are #@markdown prokaryotic then set `is_prokaryotic` to `True`. Set to `False` #@markdown otherwise or if the origin is unknown. is_prokaryote = False #@param {type:&quot;boolean&quot;} MIN_SINGLE_SEQUENCE_LENGTH = 16 MAX_SINGLE_SEQUENCE_LENGTH = 2500 MAX_MULTIMER_LENGTH = 2500 # Validate the input. sequences, model_type_to_use = notebook_utils.validate_input( input_sequences=input_sequences, min_length=MIN_SINGLE_SEQUENCE_LENGTH, max_length=MAX_SINGLE_SEQUENCE_LENGTH, max_multimer_length=MAX_MULTIMER_LENGTH) . #@markdown Once this cell has been executed, you will see #@markdown statistics about the multiple sequence alignment #@markdown (MSA) that will be used by AlphaFold. In particular, #@markdown you’ll see how well each residue is covered by similar #@markdown sequences in the MSA. # Python imports import collections import copy from concurrent import futures import json import random from urllib import request from google.colab import files from matplotlib import gridspec import matplotlib.pyplot as plt import numpy as np import py3Dmol from alphafold.model import model from alphafold.model import config from alphafold.model import data from alphafold.data import feature_processing from alphafold.data import msa_pairing from alphafold.data import parsers from alphafold.data import pipeline from alphafold.data import pipeline_multimer from alphafold.data.tools import jackhmmer from alphafold.common import protein from alphafold.relax import relax from alphafold.relax import utils from IPython import display from ipywidgets import GridspecLayout from ipywidgets import Output # Color bands for visualizing plddt PLDDT_BANDS = [(0, 50, &#39;#FF7D45&#39;), (50, 70, &#39;#FFDB13&#39;), (70, 90, &#39;#65CBF3&#39;), (90, 100, &#39;#0053D6&#39;)] # Find the closest source test_url_pattern = &#39;https://storage.googleapis.com/alphafold-colab{:s}/latest/uniref90_2021_03.fasta.1&#39; ex = futures.ThreadPoolExecutor(3) def fetch(source): request.urlretrieve(test_url_pattern.format(source)) return source fs = [ex.submit(fetch, source) for source in [&#39;&#39;, &#39;-europe&#39;, &#39;-asia&#39;]] source = None for f in futures.as_completed(fs): source = f.result() ex.shutdown() break JACKHMMER_BINARY_PATH = &#39;/usr/bin/jackhmmer&#39; DB_ROOT_PATH = f&#39;https://storage.googleapis.com/alphafold-colab{source}/latest/&#39; # The z_value is the number of sequences in a database. MSA_DATABASES = [ {&#39;db_name&#39;: &#39;uniref90&#39;, &#39;db_path&#39;: f&#39;{DB_ROOT_PATH}uniref90_2021_03.fasta&#39;, &#39;num_streamed_chunks&#39;: 59, &#39;z_value&#39;: 135_301_051}, {&#39;db_name&#39;: &#39;smallbfd&#39;, &#39;db_path&#39;: f&#39;{DB_ROOT_PATH}bfd-first_non_consensus_sequences.fasta&#39;, &#39;num_streamed_chunks&#39;: 17, &#39;z_value&#39;: 65_984_053}, {&#39;db_name&#39;: &#39;mgnify&#39;, &#39;db_path&#39;: f&#39;{DB_ROOT_PATH}mgy_clusters_2019_05.fasta&#39;, &#39;num_streamed_chunks&#39;: 71, &#39;z_value&#39;: 304_820_129}, ] # Search UniProt and construct the all_seq features only for heteromers, not homomers. if model_type_to_use == notebook_utils.ModelType.MULTIMER and len(set(sequences)) &gt; 1: MSA_DATABASES.extend([ # Swiss-Prot and TrEMBL are concatenated together as UniProt. {&#39;db_name&#39;: &#39;uniprot&#39;, &#39;db_path&#39;: f&#39;{DB_ROOT_PATH}uniprot_2021_03.fasta&#39;, &#39;num_streamed_chunks&#39;: 98, &#39;z_value&#39;: 219_174_961 + 565_254}, ]) TOTAL_JACKHMMER_CHUNKS = sum([cfg[&#39;num_streamed_chunks&#39;] for cfg in MSA_DATABASES]) MAX_HITS = { &#39;uniref90&#39;: 10_000, &#39;smallbfd&#39;: 5_000, &#39;mgnify&#39;: 501, &#39;uniprot&#39;: 50_000, } def get_msa(fasta_path): &quot;&quot;&quot;Searches for MSA for the given sequence using chunked Jackhmmer search.&quot;&quot;&quot; # Run the search against chunks of genetic databases (since the genetic # databases don&#39;t fit in Colab disk). raw_msa_results = collections.defaultdict(list) with tqdm.notebook.tqdm(total=TOTAL_JACKHMMER_CHUNKS, bar_format=TQDM_BAR_FORMAT) as pbar: def jackhmmer_chunk_callback(i): pbar.update(n=1) for db_config in MSA_DATABASES: db_name = db_config[&#39;db_name&#39;] pbar.set_description(f&#39;Searching {db_name}&#39;) jackhmmer_runner = jackhmmer.Jackhmmer( binary_path=JACKHMMER_BINARY_PATH, database_path=db_config[&#39;db_path&#39;], get_tblout=True, num_streamed_chunks=db_config[&#39;num_streamed_chunks&#39;], streaming_callback=jackhmmer_chunk_callback, z_value=db_config[&#39;z_value&#39;]) # Group the results by database name. raw_msa_results[db_name].extend(jackhmmer_runner.query(fasta_path)) return raw_msa_results features_for_chain = {} raw_msa_results_for_sequence = {} for sequence_index, sequence in enumerate(sequences, start=1): print(f&#39; nGetting MSA for sequence {sequence_index}&#39;) fasta_path = f&#39;target_{sequence_index}.fasta&#39; with open(fasta_path, &#39;wt&#39;) as f: f.write(f&#39;&gt;query n{sequence}&#39;) # Don&#39;t do redundant work for multiple copies of the same chain in the multimer. if sequence not in raw_msa_results_for_sequence: raw_msa_results = get_msa(fasta_path=fasta_path) raw_msa_results_for_sequence[sequence] = raw_msa_results else: raw_msa_results = copy.deepcopy(raw_msa_results_for_sequence[sequence]) # Extract the MSAs from the Stockholm files. # NB: deduplication happens later in pipeline.make_msa_features. single_chain_msas = [] uniprot_msa = None for db_name, db_results in raw_msa_results.items(): merged_msa = notebook_utils.merge_chunked_msa( results=db_results, max_hits=MAX_HITS.get(db_name)) if merged_msa.sequences and db_name != &#39;uniprot&#39;: single_chain_msas.append(merged_msa) msa_size = len(set(merged_msa.sequences)) print(f&#39;{msa_size} unique sequences found in {db_name} for sequence {sequence_index}&#39;) elif merged_msa.sequences and db_name == &#39;uniprot&#39;: uniprot_msa = merged_msa notebook_utils.show_msa_info(single_chain_msas=single_chain_msas, sequence_index=sequence_index) # Turn the raw data into model features. feature_dict = {} feature_dict.update(pipeline.make_sequence_features( sequence=sequence, description=&#39;query&#39;, num_res=len(sequence))) feature_dict.update(pipeline.make_msa_features(msas=single_chain_msas)) # We don&#39;t use templates in AlphaFold Colab notebook, add only empty placeholder features. feature_dict.update(notebook_utils.empty_placeholder_template_features( num_templates=0, num_res=len(sequence))) # Construct the all_seq features only for heteromers, not homomers. if model_type_to_use == notebook_utils.ModelType.MULTIMER and len(set(sequences)) &gt; 1: valid_feats = msa_pairing.MSA_FEATURES + ( &#39;msa_uniprot_accession_identifiers&#39;, &#39;msa_species_identifiers&#39;, ) all_seq_features = { f&#39;{k}_all_seq&#39;: v for k, v in pipeline.make_msa_features([uniprot_msa]).items() if k in valid_feats} feature_dict.update(all_seq_features) features_for_chain[protein.PDB_CHAIN_IDS[sequence_index - 1]] = feature_dict # Do further feature post-processing depending on the model type. if model_type_to_use == notebook_utils.ModelType.MONOMER: np_example = features_for_chain[protein.PDB_CHAIN_IDS[0]] elif model_type_to_use == notebook_utils.ModelType.MULTIMER: all_chain_features = {} for chain_id, chain_features in features_for_chain.items(): all_chain_features[chain_id] = pipeline_multimer.convert_monomer_features( chain_features, chain_id) all_chain_features = pipeline_multimer.add_assembly_features(all_chain_features) np_example = feature_processing.pair_and_merge( all_chain_features=all_chain_features, is_prokaryote=is_prokaryote) # Pad MSA to avoid zero-sized extra_msa. np_example = pipeline_multimer.pad_msa(np_example, min_num_seq=512) . #@markdown Once this cell has been executed, a zip-archive with #@markdown the obtained prediction will be automatically downloaded #@markdown to your computer. #@markdown In case you are having issues with the relaxation stage, you can disable it below. #@markdown Warning: This means that the prediction might have distracting #@markdown small stereochemical violations. run_relax = True #@param {type:&quot;boolean&quot;} # Run the model if model_type_to_use == notebook_utils.ModelType.MONOMER: model_names = config.MODEL_PRESETS[&#39;monomer&#39;] + (&#39;model_2_ptm&#39;,) elif model_type_to_use == notebook_utils.ModelType.MULTIMER: model_names = config.MODEL_PRESETS[&#39;multimer&#39;] output_dir = &#39;prediction&#39; os.makedirs(output_dir, exist_ok=True) plddts = {} ranking_confidences = {} pae_outputs = {} unrelaxed_proteins = {} with tqdm.notebook.tqdm(total=len(model_names) + 1, bar_format=TQDM_BAR_FORMAT) as pbar: for model_name in model_names: pbar.set_description(f&#39;Running {model_name}&#39;) cfg = config.model_config(model_name) if model_type_to_use == notebook_utils.ModelType.MONOMER: cfg.data.eval.num_ensemble = 1 elif model_type_to_use == notebook_utils.ModelType.MULTIMER: cfg.model.num_ensemble_eval = 1 params = data.get_model_haiku_params(model_name, &#39;./alphafold/data&#39;) model_runner = model.RunModel(cfg, params) processed_feature_dict = model_runner.process_features(np_example, random_seed=0) prediction = model_runner.predict(processed_feature_dict, random_seed=random.randrange(sys.maxsize)) mean_plddt = prediction[&#39;plddt&#39;].mean() if model_type_to_use == notebook_utils.ModelType.MONOMER: if &#39;predicted_aligned_error&#39; in prediction: pae_outputs[model_name] = (prediction[&#39;predicted_aligned_error&#39;], prediction[&#39;max_predicted_aligned_error&#39;]) else: # Monomer models are sorted by mean pLDDT. Do not put monomer pTM models here as they # should never get selected. ranking_confidences[model_name] = prediction[&#39;ranking_confidence&#39;] plddts[model_name] = prediction[&#39;plddt&#39;] elif model_type_to_use == notebook_utils.ModelType.MULTIMER: # Multimer models are sorted by pTM+ipTM. ranking_confidences[model_name] = prediction[&#39;ranking_confidence&#39;] plddts[model_name] = prediction[&#39;plddt&#39;] pae_outputs[model_name] = (prediction[&#39;predicted_aligned_error&#39;], prediction[&#39;max_predicted_aligned_error&#39;]) # Set the b-factors to the per-residue plddt. final_atom_mask = prediction[&#39;structure_module&#39;][&#39;final_atom_mask&#39;] b_factors = prediction[&#39;plddt&#39;][:, None] * final_atom_mask unrelaxed_protein = protein.from_prediction( processed_feature_dict, prediction, b_factors=b_factors, remove_leading_feature_dimension=( model_type_to_use == notebook_utils.ModelType.MONOMER)) unrelaxed_proteins[model_name] = unrelaxed_protein # Delete unused outputs to save memory. del model_runner del params del prediction pbar.update(n=1) # AMBER relax the best model # Find the best model according to the mean pLDDT. best_model_name = max(ranking_confidences.keys(), key=lambda x: ranking_confidences[x]) if run_relax: pbar.set_description(f&#39;AMBER relaxation&#39;) amber_relaxer = relax.AmberRelaxation( max_iterations=0, tolerance=2.39, stiffness=10.0, exclude_residues=[], max_outer_iterations=3) relaxed_pdb, _, _ = amber_relaxer.process(prot=unrelaxed_proteins[best_model_name]) else: print(&#39;Warning: Running without the relaxation stage.&#39;) relaxed_pdb = protein.to_pdb(unrelaxed_proteins[best_model_name]) pbar.update(n=1) # Finished AMBER relax. # Construct multiclass b-factors to indicate confidence bands # 0=very low, 1=low, 2=confident, 3=very high banded_b_factors = [] for plddt in plddts[best_model_name]: for idx, (min_val, max_val, _) in enumerate(PLDDT_BANDS): if plddt &gt;= min_val and plddt &lt;= max_val: banded_b_factors.append(idx) break banded_b_factors = np.array(banded_b_factors)[:, None] * final_atom_mask to_visualize_pdb = utils.overwrite_b_factors(relaxed_pdb, banded_b_factors) # Write out the prediction pred_output_path = os.path.join(output_dir, &#39;selected_prediction.pdb&#39;) with open(pred_output_path, &#39;w&#39;) as f: f.write(relaxed_pdb) # Visualise the prediction &amp; confidence show_sidechains = True def plot_plddt_legend(): &quot;&quot;&quot;Plots the legend for pLDDT.&quot;&quot;&quot; thresh = [&#39;Very low (pLDDT &lt; 50)&#39;, &#39;Low (70 &gt; pLDDT &gt; 50)&#39;, &#39;Confident (90 &gt; pLDDT &gt; 70)&#39;, &#39;Very high (pLDDT &gt; 90)&#39;] colors = [x[2] for x in PLDDT_BANDS] plt.figure(figsize=(2, 2)) for c in colors: plt.bar(0, 0, color=c) plt.legend(thresh, frameon=False, loc=&#39;center&#39;, fontsize=20) plt.xticks([]) plt.yticks([]) ax = plt.gca() ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) plt.title(&#39;Model Confidence&#39;, fontsize=20, pad=20) return plt # Show the structure coloured by chain if the multimer model has been used. if model_type_to_use == notebook_utils.ModelType.MULTIMER: multichain_view = py3Dmol.view(width=800, height=600) multichain_view.addModelsAsFrames(to_visualize_pdb) multichain_style = {&#39;cartoon&#39;: {&#39;colorscheme&#39;: &#39;chain&#39;}} multichain_view.setStyle({&#39;model&#39;: -1}, multichain_style) multichain_view.zoomTo() multichain_view.show() # Color the structure by per-residue pLDDT color_map = {i: bands[2] for i, bands in enumerate(PLDDT_BANDS)} view = py3Dmol.view(width=800, height=600) view.addModelsAsFrames(to_visualize_pdb) style = {&#39;cartoon&#39;: {&#39;colorscheme&#39;: {&#39;prop&#39;: &#39;b&#39;, &#39;map&#39;: color_map}}} if show_sidechains: style[&#39;stick&#39;] = {} view.setStyle({&#39;model&#39;: -1}, style) view.zoomTo() grid = GridspecLayout(1, 2) out = Output() with out: view.show() grid[0, 0] = out out = Output() with out: plot_plddt_legend().show() grid[0, 1] = out display.display(grid) # Display pLDDT and predicted aligned error (if output by the model). if pae_outputs: num_plots = 2 else: num_plots = 1 plt.figure(figsize=[8 * num_plots, 6]) plt.subplot(1, num_plots, 1) plt.plot(plddts[best_model_name]) plt.title(&#39;Predicted LDDT&#39;) plt.xlabel(&#39;Residue&#39;) plt.ylabel(&#39;pLDDT&#39;) if num_plots == 2: plt.subplot(1, 2, 2) pae, max_pae = list(pae_outputs.values())[0] plt.imshow(pae, vmin=0., vmax=max_pae, cmap=&#39;Greens_r&#39;) plt.colorbar(fraction=0.046, pad=0.04) # Display lines at chain boundaries. best_unrelaxed_prot = unrelaxed_proteins[best_model_name] total_num_res = best_unrelaxed_prot.residue_index.shape[-1] chain_ids = best_unrelaxed_prot.chain_index for chain_boundary in np.nonzero(chain_ids[:-1] - chain_ids[1:]): if chain_boundary.size: plt.plot([0, total_num_res], [chain_boundary, chain_boundary], color=&#39;red&#39;) plt.plot([chain_boundary, chain_boundary], [0, total_num_res], color=&#39;red&#39;) plt.title(&#39;Predicted Aligned Error&#39;) plt.xlabel(&#39;Scored residue&#39;) plt.ylabel(&#39;Aligned residue&#39;) # Save the predicted aligned error (if it exists). pae_output_path = os.path.join(output_dir, &#39;predicted_aligned_error.json&#39;) if pae_outputs: # Save predicted aligned error in the same format as the AF EMBL DB. pae_data = notebook_utils.get_pae_json(pae=pae, max_pae=max_pae.item()) with open(pae_output_path, &#39;w&#39;) as f: f.write(pae_data) # Download the predictions !zip -q -r {output_dir}.zip {output_dir} files.download(f&#39;{output_dir}.zip&#39;) . Interpreting the prediction . In general predicted LDDT (pLDDT) is best used for intra-domain confidence, whereas Predicted Aligned Error (PAE) is best used for determining between domain or between chain confidence. . Please see the AlphaFold methods paper, the AlphaFold predictions of the human proteome paper, and the AlphaFold-Multimer paper as well as our FAQ on how to interpret AlphaFold predictions. . FAQ &amp; Troubleshooting . How do I get a predicted protein structure for my protein? Click on the Connect button on the top right to get started. | Paste the amino acid sequence of your protein (without any headers) into the “Enter the amino acid sequence to fold”. | Run all cells in the Colab, either by running them individually (with the play button on the left side) or via Runtime &gt; Run all. | The predicted protein structure will be downloaded once all cells have been executed. Note: This can take minutes to hours - see below. | . | How long will this take? Downloading the AlphaFold source code can take up to a few minutes. | Downloading and installing the third-party software can take up to a few minutes. | The search against genetic databases can take minutes to hours. | Running AlphaFold and generating the prediction can take minutes to hours, depending on the length of your protein and on which GPU-type Colab has assigned you. | . | My Colab no longer seems to be doing anything, what should I do? Some steps may take minutes to hours to complete. | If nothing happens or if you receive an error message, try restarting your Colab runtime via Runtime &gt; Restart runtime. | If this doesn’t help, try resetting your Colab runtime via Runtime &gt; Factory reset runtime. | . | How does this compare to the open-source version of AlphaFold? This Colab version of AlphaFold searches a selected portion of the BFD dataset and currently doesn’t use templates, so its accuracy is reduced in comparison to the full version of AlphaFold that is described in the AlphaFold paper and Github repo (the full version is available via the inference script). | . | What is a Colab? See the Colab FAQ. | . | I received a warning “Notebook requires high RAM”, what do I do? The resources allocated to your Colab vary. See the Colab FAQ for more details. | You can execute the Colab nonetheless. | . | I received an error “Colab CPU runtime not supported” or “No GPU/TPU found”, what do I do? Colab CPU runtime is not supported. Try changing your runtime via Runtime &gt; Change runtime type &gt; Hardware accelerator &gt; GPU. | The type of GPU allocated to your Colab varies. See the Colab FAQ for more details. | If you receive “Cannot connect to GPU backend”, you can try again later to see if Colab allocates you a GPU. | Colab Pro offers priority access to GPUs. | . | I received an error “ModuleNotFoundError: No module named ...”, even though I ran the cell that imports it, what do I do? Colab notebooks on the free tier time out after a certain amount of time. See the Colab FAQ. Try rerunning the whole notebook from the beginning. | . | Does this tool install anything on my computer? No, everything happens in the cloud on Google Colab. | At the end of the Colab execution a zip-archive with the obtained prediction will be automatically downloaded to your computer. | . | How should I share feedback and bug reports? Please share any feedback and bug reports as an issue on Github. | . | . Related work . Take a look at these Colab notebooks provided by the community (please note that these notebooks may vary from our validated AlphaFold system and we cannot guarantee their accuracy): . The ColabFold AlphaFold2 notebook by Sergey Ovchinnikov, Milot Mirdita and Martin Steinegger, which uses an API hosted at the Södinglab based on the MMseqs2 server (Mirdita et al. 2019, Bioinformatics) for the multiple sequence alignment creation. | . License and Disclaimer . This is not an officially-supported Google product. . This Colab notebook and other information provided is for theoretical modelling only, caution should be exercised in its use. It is provided ‘as-is’ without any warranty of any kind, whether expressed or implied. Information is not intended to be a substitute for professional medical advice, diagnosis, or treatment, and does not constitute medical or other professional advice. . Copyright 2021 DeepMind Technologies Limited. . AlphaFold Code License . Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0. . Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. . Model Parameters License . The AlphaFold parameters are made available for non-commercial use only, under the terms of the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. You can find details at: https://creativecommons.org/licenses/by-nc/4.0/legalcode . Third-party software . Use of the third-party software, libraries or code referred to in the Acknowledgements section in the AlphaFold README may be governed by separate terms and conditions or license provisions. Your use of the third-party software, libraries or code is subject to any such terms and you should check that you can comply with any applicable restrictions or terms and conditions before use. . Mirrored Databases . The following databases have been mirrored by DeepMind, and are available with reference to the following: . UniProt: v2021_03 (unmodified), by The UniProt Consortium, available under a Creative Commons Attribution-NoDerivatives 4.0 International License. | UniRef90: v2021_03 (unmodified), by The UniProt Consortium, available under a Creative Commons Attribution-NoDerivatives 4.0 International License. | MGnify: v2019_05 (unmodified), by Mitchell AL et al., available free of all copyright restrictions and made fully and freely available for both non-commercial and commercial use under CC0 1.0 Universal (CC0 1.0) Public Domain Dedication. | BFD: (modified), by Steinegger M. and Söding J., modified by DeepMind, available under a Creative Commons Attribution-ShareAlike 4.0 International License. See the Methods section of the AlphaFold proteome paper for details. | .",
            "url": "https://j143.github.io/tech-insights/2022/03/27/AlphaFold.html",
            "relUrl": "/2022/03/27/AlphaFold.html",
            "date": " • Mar 27, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Title",
            "content": "Copyright &copy; 2020 The Apache Software Foundation. . #- # # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # &quot;License&quot;); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. # #- . Developer notebook for Apache SystemDS . Run this notebook online at Google Colab ↗. . This Jupyter/Colab-based tutorial will interactively walk through development setup and running SystemDS in both the . A. standalone mode B. with Apache Spark. . Flow of the notebook: . Download and Install the dependencies | Go to section A or B | Download and Install the dependencies . Runtime: Java (OpenJDK 8 is preferred) | Build: Apache Maven | Backend: Apache Spark (optional) | Setup . A custom function to run OS commands. . def run(command): print(&#39;&gt;&gt; {}&#39;.format(command)) !{command} print(&#39;&#39;) . Install Java . Let us install OpenJDK 8. More about OpenJDK ↗. . !apt-get update !apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null # run the below command to replace the existing installation !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java import os os.environ[&quot;JAVA_HOME&quot;] = &quot;/usr/lib/jvm/java-8-openjdk-amd64&quot; !java -version . Install Apache Maven . SystemDS uses Apache Maven to build and manage the project. More about Apache Maven ↗. . Maven builds SystemDS using its project object model (POM) and a set of plugins. One would find pom.xml find the codebase! . maven_version = &#39;apache-maven-3.6.3&#39; maven_path = f&quot;/opt/{maven_version}&quot; if not os.path.exists(maven_path): run(f&quot;wget -q -nc -O apache-maven.zip https://downloads.apache.org/maven/maven-3/3.6.3/binaries/{maven_version}-bin.zip&quot;) run(&#39;unzip -q -d /opt apache-maven.zip&#39;) run(&#39;rm -f apache-maven.zip&#39;) # Let&#39;s choose the absolute path instead of $PATH environment variable. def maven(args): run(f&quot;{maven_path}/bin/mvn {args}&quot;) maven(&#39;-v&#39;) . Install Apache Spark (Optional, if you want to work with spark backend) . NOTE: If spark is not downloaded. Let us make sure the version we are trying to download is officially supported at https://spark.apache.org/downloads.html . spark_version = &#39;spark-2.4.7&#39; hadoop_version = &#39;hadoop2.7&#39; spark_path = f&quot;/opt/{spark_version}-bin-{hadoop_version}&quot; if not os.path.exists(spark_path): run(f&quot;wget -q -nc -O apache-spark.tgz https://downloads.apache.org/spark/{spark_version}/{spark_version}-bin-{hadoop_version}.tgz&quot;) run(&#39;tar zxfv apache-spark.tgz -C /opt&#39;) run(&#39;rm -f apache-spark.tgz&#39;) os.environ[&quot;SPARK_HOME&quot;] = spark_path os.environ[&quot;PATH&quot;] += &quot;:$SPARK_HOME/bin&quot; . Get Apache SystemDS . Apache SystemDS development happens on GitHub at apache/systemds ↗ . !git clone https://github.com/apache/systemds systemds --depth=1 %cd systemds . Build the project . # Option 1: Build only the java codebase maven(&#39;clean package -q&#39;) # Option 2: For building along with python distribution # maven(&#39;clean package -P distribution&#39;) . A. Working with SystemDS in standalone mode . NOTE: Let&#39;s pay attention to directories and relative paths. :) . 1. Set SystemDS environment variables . These are useful for the ./bin/systemds script. . !export SYSTEMDS_ROOT=$(pwd) !export PATH=$SYSTEMDS_ROOT/bin:$PATH . 2. Download Haberman data . Data source: https://archive.ics.uci.edu/ml/datasets/Haberman&#39;s+Survival . About: The survival of patients who had undergone surgery for breast cancer. . Data Attributes: . Age of patient at time of operation (numerical) | Patient&#39;s year of operation (year - 1900, numerical) | Number of positive axillary nodes detected (numerical) | Survival status (class attribute) 1 = the patient survived 5 years or longer | 2 = the patient died within 5 year | . | !mkdir ../data . !wget -P ../data/ https://web.archive.org/web/20200725014530/https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data . # Notice that the test is plain csv with no headers! !sed -n 1,10p ../data/haberman.data . 2.1 Set metadata for the data . The data does not have any info on the value types. So, metadata for the data helps know the size and format for the matrix data as .mtd file with the same name and location as .data file. . !echo &#39;{&quot;rows&quot;: 306, &quot;cols&quot;: 4, &quot;format&quot;: &quot;csv&quot;}&#39; &gt; ../data/haberman.data.mtd # generate type description for the data !echo &#39;1,1,1,2&#39; &gt; ../data/types.csv !echo &#39;{&quot;rows&quot;: 1, &quot;cols&quot;: 4, &quot;format&quot;: &quot;csv&quot;}&#39; &gt; ../data/types.csv.mtd . 3. Find the algorithm to run with systemds . !ls . !ls scripts/algorithms . # Output the algorithm documentation # start from line no. 22 onwards. Till 35th line the command looks like !sed -n 22,35p ./scripts/algorithms/Univar-Stats.dml . !./bin/systemds ./scripts/algorithms/Univar-Stats.dml -nvargs X=../data/haberman.data TYPES=../data/types.csv STATS=../data/univarOut.mtx CONSOLE_OUTPUT=TRUE . 3.1 Let us inspect the output data . !sed -n 1,10p ../data/univarOut.mtx . B. Run SystemDS with Apache Spark . Playground for DML scripts . DML - A custom language designed for SystemDS with R-like syntax. . A test dml script to prototype algorithms . Modify the code in the below cell and run to work develop data science tasks in a high level language. . %%writefile ../test.dml # This code code acts as a playground for dml code X = rand (rows = 20, cols = 10) y = X %*% rand(rows = ncol(X), cols = 1) lm(X = X, y = y) . Submit the dml script to Spark with spark-submit. More about Spark Submit ↗ . !$SPARK_HOME/bin/spark-submit ./target/SystemDS.jar -f ../test.dml . Run a binary classification example with sample data . One would notice that no other script than simple dml is used in this example completely. . # !$SPARK_HOME/bin/spark-submit ./target/SystemDS.jar -f ./scripts/nn/examples/fm-binclass-dummy-data.dml .",
            "url": "https://j143.github.io/tech-insights/2021/05/17/systemds-on-colaboratory.html",
            "relUrl": "/2021/05/17/systemds-on-colaboratory.html",
            "date": " • May 17, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://j143.github.io/tech-insights/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://j143.github.io/tech-insights/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://j143.github.io/tech-insights/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://j143.github.io/tech-insights/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}